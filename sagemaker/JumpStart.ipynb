{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2dc68-8175-4081-96aa-d33318a03b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker JumpStart - deploy text generation model\n",
    "\n",
    "This notebook demonstrates how to use the SageMaker Python SDK to deploy a SageMaker JumpStart text generation model and invoke the endpoint.\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "Select your desired model ID. You can search for available models in the [Built-in Algorithms with pre-trained Model Table](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html).\n",
    "model_id = \"huggingface-llm-mistral-7b-instruct\"\n",
    "If your selected model is gated, you will need to set `accept_eula` to True to accept the model end-user license agreement (EULA).\n",
    "accept_eula = False\n",
    "## Deploy model\n",
    "Using the model ID, define your model as a JumpStart model. You can deploy the model on other instance types by passing `instance_type` to `JumpStartModel`. See [Deploy publicly available foundation models with the JumpStartModel class](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-use-python-sdk.html#jumpstart-foundation-models-use-python-sdk-model-class) for more configuration options.\n",
    "model = JumpStartModel(model_id=model_id)\n",
    "You can now deploy your JumpStart model. The deployment might take few minutes.\n",
    "predictor = model.deploy(accept_eula=accept_eula)\n",
    "## Invoke endpoint\n",
    "Programmatically retrieve example playloads from the `JumpStartModel` object.\n",
    "example_payloads = model.retrieve_all_examples()\n",
    "Now you can invoke the endpoint for each retrieved example payload.\n",
    "for payload in example_payloads:\n",
    "    response = predictor.predict(payload.body)\n",
    "    response = response[0] if isinstance(response, list) else response\n",
    "    print(\"Input:\\n\", payload.body, end=\"\\n\\n\")\n",
    "    print(\"Output:\\n\", response[\"generated_text\"].strip(), end=\"\\n\\n\\n\")\n",
    "This model supports the following common payload parameters. You may specify any subset of these parameters when invoking an endpoint.\n",
    "\n",
    "* **do_sample:** If True, activates logits sampling. If specified, it must be boolean.\n",
    "* **max_new_tokens:** Maximum number of generated tokens. If specified, it must be a positive integer.\n",
    "* **repetition_penalty:** A penalty for repetitive generated text. 1.0 means no penalty.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "* **seed**: Random sampling seed.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **details:** Return generation details, to include output token logprobs and IDs.\n",
    "\n",
    "The model will also support additional payload parameters that are dependent on the image used for this model. You can find the default image by inspecting `model.image_uri`. For information on additional payload parameters, view [LMI input output schema](https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/lmi_input_output_schema.html) or, for text generation inference (TGI), see the following list.\n",
    "* **stop**: If specified, it must a list of strings. Text generation stops if any one of the specified strings is generated.\n",
    "* **truncate:** Truncate inputs tokens to the given size.\n",
    "* **typical_p:** Typical decoding mass, according to [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666).\n",
    "* **best_of:** Generate best_of sequences and return the one if the highest token logprobs.\n",
    "* **watermark:** Whether to perform watermarking with [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226).\n",
    "* **decoder_input_details:** Return decoder input token logprobs and IDs.\n",
    "* **top_n_tokens:** Return the N most likely tokens at each step.\n",
    "## Clean up\n",
    "predictor.delete_predictor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
